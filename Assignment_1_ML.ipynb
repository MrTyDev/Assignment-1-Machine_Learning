{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import Libraries\n",
    "\n",
    "In this section, we import the necessary libraries for data manipulation, visualization, and machine learning.\n",
    "\n",
    "- `numpy`: A fundamental package for scientific computing with Python.\n",
    "- `matplotlib.pyplot`: A plotting library for creating static, animated, and interactive visualizations in Python.\n",
    "- `pandas`: A powerful data manipulation and analysis library for Python.\n",
    "- `seaborn`: A data visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "- `sklearn.datasets`: A module in scikit-learn that provides utilities to load datasets.\n",
    "- `sklearn.model_selection.train_test_split`: A function to split the dataset into training and testing sets.\n",
    "- `sklearn.linear_model.LogisticRegression`: A module to perform logistic regression.\n",
    "- `sklearn.metrics.mean_squared_error`: A function to compute the mean squared error, a measure of the quality of an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import imblearn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Plot Quality Distribution and print quanity of distributions\n",
    "\n",
    "Visualize the distribution of wine quality for both red and white wines.\n",
    "\n",
    "- `plt.figure(figsize=(10, 6))`: Create a new figure with a specified size.\n",
    "- `sns.countplot(x='quality', hue='type', data=df, palette={'Red': 'red', 'White': 'grey'})`: Create a count plot to show the distribution of wine quality, with different colors for red and white wines.\n",
    "- `plt.xlabel('Quality')`: Set the label for the x-axis.\n",
    "- `plt.ylabel('Count')`: Set the label for the y-axis.\n",
    "- `plt.title('Quality Distribution of Red and White Wines')`: Set the title of the plot.\n",
    "- `plt.show()`: Display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_red_wine = pd.read_csv('winequality-red.csv', sep=';')\n",
    "df_white_wine = pd.read_csv('winequality-white.csv', sep=';')\n",
    "\n",
    "# Add a column to distinguish between red and white wine\n",
    "df_red_wine['type'] = 'Red'\n",
    "df_white_wine['type'] = 'White'\n",
    "\n",
    "# Concatenate the dataframes\n",
    "df = pd.concat([df_red_wine, df_white_wine])\n",
    "\n",
    "# Plot the quality distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='quality', hue='type', data=df, palette={'Red': 'red', 'White': 'grey'})\n",
    "plt.xlabel('Quality')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Quality Distribution of Red and White Wines')\n",
    "plt.show()\n",
    "\n",
    "print(df_red_wine['quality'].value_counts())\n",
    "print(df_white_wine['quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what we inspected in the datasets we will continue with the white wine dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split White Wine Dataset into Train and Test Sets\n",
    "\n",
    "In this section, we split the white wine dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target\n",
    "X = df_white_wine.drop(['quality', 'type'], axis=1)\n",
    "y = df_white_wine['quality']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform Scaling on the Data\n",
    "\n",
    "In this section, we will perform scaling on the white wine dataset after splitting it into training and testing sets.\n",
    "\n",
    "- `train_test_split`: A function to split the dataset into training and testing sets.\n",
    "- `StandardScaler()`: Initialize the scaler to standardize features by removing the mean and scaling to unit variance.\n",
    "- `scaler.fit_transform(X_train)`: Fit the scaler on the training data and transform it.\n",
    "- `scaler.transform(X_test)`: Transform the testing data using the fitted scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both train and test sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled data back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classifier Evaluation\n",
    "\n",
    "In this section, we will evaluate the performance of four classifiers: Logistic Regression, Decision Trees, Support Vector Machines (SVM), and K-Nearest Neighbors (KNN) using Repeated k-Fold Cross-Validation.\n",
    "\n",
    "- `RepeatedStratifiedKFold(n_splits=3, n_repeats=10, random_state=42)`: Initialize the cross-validation strategy.\n",
    "- `cross_validate`: Perform cross-validation and evaluate the classifiers using metrics such as accuracy, precision, recall, and F1-score.\n",
    "- `LogisticRegression()`: Initialize the Logistic Regression classifier.\n",
    "- `DecisionTreeClassifier()`: Initialize the Decision Tree classifier.\n",
    "- `SVC()`: Initialize the Support Vector Machine classifier.\n",
    "- `KNeighborsClassifier()`: Initialize the K-Nearest Neighbors classifier.\n",
    "- `np.mean(scores['test_accuracy'])`: Calculate the mean accuracy score.\n",
    "- `np.std(scores['test_accuracy'])`: Calculate the standard deviation of the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    'liner regression': LogisticRegression(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=10, random_state=42)\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Evaluate each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    scores = cross_validate(clf, X_train_scaled, y_train, cv=cv, scoring=scoring)\n",
    "    results[name] = {\n",
    "        'accuracy_mean': np.mean(scores['test_accuracy']),\n",
    "        'accuracy_std': np.std(scores['test_accuracy']),\n",
    "        'precision_mean': np.mean(scores['test_precision']),\n",
    "        'precision_std': np.std(scores['test_precision']),\n",
    "        'recall_mean': np.mean(scores['test_recall']),\n",
    "        'recall_std': np.std(scores['test_recall']),\n",
    "        'f1_mean': np.mean(scores['test_f1']),\n",
    "        'f1_std': np.std(scores['test_f1'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy_mean']:.4f} ± {metrics['accuracy_std']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision_mean']:.4f} ± {metrics['precision_std']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall_mean']:.4f} ± {metrics['recall_std']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['f1_mean']:.4f} ± {metrics['f1_std']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Identify the best classifier based on the chosen metric (e.g., F1 Score)\n",
    "best_classifier = max(results, key=lambda name: results[name]['f1_mean'])\n",
    "print(f\"The best classifier based on F1 Score is: {best_classifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Final Model\n",
    "\n",
    "In this section, we train the final model using the best-performing classifier identified in the previous step.\n",
    "\n",
    "- `classifiers[best_classifier]`: Retrieve the best classifier from the dictionary of classifiers.\n",
    "- `clf.fit(X_train_scaled, y_train)`: Train the best classifier on the entire training set.\n",
    "\n",
    "This step ensures that the final model is trained on the full training dataset to maximize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize the best classifier\n",
    "clf = classifiers[best_classifier]\n",
    "\n",
    "# Train the best classifier on the entire training set\n",
    "clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Report the Model's Performance on the Test Set\n",
    "\n",
    "In this section, we report the performance of the final model on the test set using the best-performing classifier identified in the previous step.\n",
    "\n",
    "- `accuracy_score(y_test, y_pred)`: Calculate the accuracy of the final model.\n",
    "- `precision_score(y_test, y_pred, average='weighted')`: Calculate the precision of the final model.\n",
    "- `recall_score(y_test, y_pred, average='weighted')`: Calculate the recall of the final model.\n",
    "- `f1_score(y_test, y_pred, average='weighted')`: Calculate the F1 score of the final model.\n",
    "\n",
    "The final model's performance metrics are as follows:\n",
    "\n",
    "- **Accuracy**: Measures the proportion of correctly classified instances.\n",
    "- **Precision**: Measures the proportion of true positive instances among the instances classified as positive.\n",
    "- **Recall**: Measures the proportion of true positive instances among the actual positive instances.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a single metric that balances both concerns.\n",
    "\n",
    "The best classifier based on F1 Score is used to build the final model, and its performance on the test set is reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the final model\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "final_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "final_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "final_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the final model performance\n",
    "print(f\"Final Model Performance ({best_classifier}):\")\n",
    "print(f\"  Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"  Precision: {final_precision:.4f}\")\n",
    "print(f\"  Recall: {final_recall:.4f}\")\n",
    "print(f\"  F1 Score: {final_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
